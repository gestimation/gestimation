---
title: "Why Logistic Regression Fails in Small Samples"
description: ""
tags:
  - Bias
  - Effect measure, 
  - Generalized linear model
format:
  html:
    toc: true
date: 2025-12-08
---

::: {.hero-header}
![](../image/father-daughter.png){.hero-header-img}
:::

# Adjusting for Bias V − Why Logistic Regression Fails in Small Samples

::: {.serif-block .text-right}
Keywords: bias, effect measure, generalized linear model
:::

------------------------------------------------------------------------

### なぜロジスティック回帰の計算ができなかったのか

::::::::::::::::::::::::::::::::: dialogue
::: voice-daughter
私「お父さん、コーヒー飲んで寛いでいるところわるいんだけど、時間あるかな。ロジスティック回帰についてなんだけど。説明してくれたのはありがたいんだけどね、群作用とか完全分離とか、途中でわかんなくなっちゃってさ。気になって、先に進めないわけよ」
:::

::: voice-father
お父さん「そうだったのか。Rコードを見せてもらったけど、解析自体は間違ってなさそうだよ」
::: 

::: voice-daughter
私「いや、私自身の問題だよね。納得感というか自信というか。だからさ、完全分離について説明してくれない？この前の解析で、なにが起きたのか知りたいんだよね。あの解が出なかったていう結果しかないと、なんだかね」
::: 
::::::::::::::::::::::::::::::::: 

::: {.callout-note appearance="simple" title="ロジスティック回帰の推定"}

**尤度関数による推定**

以前、ロジスティック回帰の$\pi_i$は2項分布の確率に対応しているといいました。2項分布の確率関数にはデータと$\pi_i$が含まれていますが、この式にデータを代入すると、$\pi_i$の式が得られます。さらに、$\pi_i$は$\beta_0,\beta_1,...,\beta_{p-1}$の関数です。したがって、確率関数の式は、$L(\beta_0,\beta_1,...,\beta_{p-1})$というように、$\beta_0,\beta_1,...,\beta_{p-1}$の関数とみなすことができます。この関数を尤度関数といいます。

ロジスティック回帰の回帰係数は、この尤度関数を最大化する値として計算されます。ただし、データから必ずしも解が求まるわけではありません。計算が収束しなかったり、不安定になったりする理由は、主に2通り考えられます。

**完全分離**

一つ目は、完全分離（complete separation）や擬似完全分離（quasi-complete separation）です。完全分離とは、

$\log(\frac{\hat\pi_i}{1-\hat\pi_i})=\hat\beta_0+\hat\beta_1X_{i,1}+...+\hat\beta_{p-1}X_{i,p-1}$

を計算したとき、その値によって、アウトカムを100%の精度で0または1に判別できてしまう状況のことをいいます。擬似完全分離は、100%ではないものの、共変量からアウトカムがほぼ判別できてしまう状況です。別の言葉でいえば、共変量の情報に比べて、データに含まれる情報が不足していて、実質的にランダム性がないということを意味します。つまり、完全分離や擬似完全分離は、サンプルサイズに比べ、共変量の数が多すぎるときによく生じる問題です。

**多重共線性**

二つ目は、多重共線性（multicollinearity）です。これは、共変量同士の相関が強すぎることを意味します。仮に、年収が年齢に完全に比例する状況（つまり相関が1）を考えてみてください。このとき、ロジスティック回帰を用いて年収と年齢の影響を分離することはできません。これが多重共線性の一例です。言い換えると、相関の高い共変量は、推定が不安定になるため、同時にロジスティック回帰に含めない方がよい場合が多いです。
:::

::::::::::::::::::::::::::::::::: dialogue
::: voice-father
お父さん「つまり、完全分離が生じるデータは、若い人は全員復職、高齢者は全員非復職っていう調査結果みたいなもの。年齢だけで復職できるかどうか100%当てられてしまう。まさにこれが完全分離で、尤度関数から解が求まらない状況だよ」
::: 

::: voice-daughter
私「あれ？回帰分析って最小2乗法を使うんじゃないの？尤度関数？」
::: 

::: voice-father
お父さん「大学の教科書ではそう教わるけどね。最小2乗法を使うのは連続データのときだけかな。別の推定方法もいろいろある」
::: 

::: voice-daughter
私「ふーん。新しい方法がいろいろありそうってのは知ってるよ。AIとか機械学習とか」
::: 

::: voice-father
お父さん「まあね。はやりだよね」
::: 

::: voice-daughter
私「統計学と機械学習ってどう違うの？同じ？」
::: 

::: voice-father
お父さん「お隣さんって感じかな。データを解析するって意味では同じことをしているわけだし、機械学習の教科書でも、ロジスティック回帰とかロジスティック判別とかは扱うもの。統計学で習うROC曲線なんかは、機械学習の範疇な気がするし。重なる部分は多いよ。まあ、機械学習の方がたくさんのパラメータを推定する傾向にあるとはいえるかな。ロジスティック回帰よりニューラルネットワークの方が、モデルも複雑だし、パラメータの数も多い。さっき共変量の数を減らそうっていったでしょ。機械学習では、ああいうパラメータの数を減らすって発想はしない」
::: 

::: voice-daughter
私「パラメータの数が少ないのが統計学の特徴ってわけ？」
::: 

::: voice-father
お父さん「うーん、でもそれは程度問題かもしれない。一番の違いはなんだろうね。統計学は確率モデルが必ず基礎にあるってことかな。そして、標本から母集団を調べるっていう考え方をする。手法でいうと、p値とか信頼区間とか、ああいうやつね」
::: 

### 統計手法を選ぶ理由

::: voice-daughter
私「はやりだから使っていいのかな。ほら、ニューラルネットワークとかこの前の群作用とか、知らなくてもみんな使ってるよね。問題ないのかな」
::: 

::: voice-father
お父さん「でもね、2項分布とロジット関数のところまでは理解できなかった？」
::: 

::: voice-daughter
私「そこまではできた」
::: 

::: voice-father
お父さん「なら合格点だと思う。数理的な手法って奥が深いでしょ。だから、仮に数学的な真実があったとして、究極的には部分的な理解しかできないかもしれない。専門家とユーザーで程度は違うけどね。だから、みんな使ってるっていうのは、使ってもいい理由のひとつにはなる。医療でも、添付文書で有効性が確立していないって書かれた薬が、第一選択だったりするでしょ」
::: 

::: voice-daughter
私「ああ、コミュニティスタンダードのことね。そういうこともあるのが現実だけど、すっきりはしてないよね、医師はみんな。それにさ、コミュニティスタンダードの薬は、単にみんな使っているから選んでるんじゃないよ。難病で他に候補がなかったり、エビデンスが足りなくても経験上は効果があるから、その薬を使って治療してるの」
:::

::: voice-father
お父さん「それって、医療上の目的に対してじゅうぶん役に立ってるってことでしょ。ロジスティック回帰も同じじゃない？用途に見合った性能があるって自信を持っていえれば、ところどころ理解が追い付かなくても、使用していいと思う。そう、まさに完璧に解明された技術なんてないのが現実だからね。世の中ではいろんな技術が使われてる。でも、すべて3つの理由で正当化されるんだよ」
:::

- 数学と自然科学が支えている
- コミュニティが支えている
- 使用する目的が支えている

::: voice-father
お父さん「それぞれが100%じゃなくても、この3つが支えあえれば使う意味があるんだ」
::: 

::: voice-daughter
私「そういうもんかな。納得したような、まだ霧が残っているような気もする。まあいいや、疲れてきたし。性別のオッズ比が無限大になった理由はわかったよ。尤度関数にデータを代入したら、回帰係数の解が変になるような式になったってわけね」
::: 
:::::::::::::::::::::::::::::::::

::: {.callout-note appearance="simple" collapse="true" title="データの型と統計手法"}

アウトカムはデータの型によって4種類に分類されます。

- 連続データ （例：血圧やQOL）
- 分類データ （例：有害事象の有無）
- 計数データ （例：有害事象の発生件数）
- 生存時間データ （例：全生存期間）

さらに、分類データはいくつかの種類があり、カテゴリが2通りのものを2値データ、カテゴリに順序があるものを順序データと呼んでいます。生存時間データの一種には、競合リスクを考慮した競合リスクデータがあります。データの型によって、正規分布や2項分布など確率分布が異なりますよね。そのため、それぞれ異なる統計手法が用いられます。詳しくは表をみてください。

仮に、ストーマ造設あり・なしと復職状況の関係を調べたいとすると、2つの変数をモデルに当てはめることになるため、回帰モデルを用いて調べたいとします。このときアウトカムを復職の有無（2値データ）とするなら、ロジスティック回帰を選ぶべきです。もし復職までの期間（生存時間データ）とするなら、Cox回帰の方が適切です。

**個人内で反復測定があるか**

同一個人にくり返し臨床検査やアンケートを行うと、似たような検査結果になりますよね。たとえば、朝と晩に血圧を測ると、2つの測定値に相関が生じます。治療前と治療後に、QOL質問票に回答してもらう場合も、その測定値は独立ではありません。このように1人の患者に複数回の測定値があるデータを**反復測定データ**といいます。反復測定データでは、個人内の測定値は独立ではありません。したがって、独立性を仮定している統計手法（たとえば対応のないt検定や$\chi^2$検定）は不適切です。反復測定データの解析では、**変量効果モデル（random-effects model）**や**一般化推定方程式（generalized estimating equation）**と呼ばれるやや高度な手法が用いられます。この場合の変量効果は、「ひとりひとりの個人の効果」を表しています。これらの方法は、個人内でデータが独立ではないこと（データの相関）を考慮するものです。

**交絡の調整が必要かどうか**

**交絡因子（confounder）**とは、治療とアウトカムとの関係を歪める第三の因子のことです。仮に、ストーマ造設あり・なしと復職状況の関係に、年齢が強くかかわっているとしましょう。そして、ストーマ保有者と非保有者で、平均年齢が異なっているとします。このような場合、2群間の年齢の違いを無視するとバイアスが生じます。これを補正するために、ロジスティック回帰がよく用いられます。 ランダム化臨床試験では、比較する群間で実験条件が揃っているため、交絡の調整は不要です。一方、観察研究（コホート研究やケース・コントロール研究）では交絡の調整は必須です。今回題材にした復職率の調査も観察研究の一種ですから、統計解析ではロジスティック回帰やCox回帰が主に用いられるでしょう。最近では交絡を調整して、正しく因果関係を調べるために、因果推論の手法（プロペンシティスコアなど）を用いることが増えてきました（田中2022）。

| データの型 | 反復測定 | 交絡調整 | 統計手法 | 備考 |
|---------------|---------------|---------------|---------------|---------------|
| 連続データ | なし | なし | 対応のない*t*検定、Wilcoxon順位和検定 | 平均の比較 |
|  | 個人内で2回測定 | なし | 対応のある*t*検定、Wilcoxon符号付順位検定 | 対応のあるデータ |
|  | なし | なし | 正規線型モデル | 回帰モデルの一種 |
|  | あり | あり | 変量効果モデル、一般化推定方程式 | 回帰モデルの一種 |
| 2値データ | なし | なし | $\chi^2$検定 | 割合の比較 |
|  | 個人内で2回測定 | あり | McNemar検定 | 対応のあるデータ |
|  | なし | あり | ロジスティック回帰 | 回帰モデルの一種 |
|  | あり | あり | 変量効果モデル、一般化推定方程式 | 回帰モデルの一種 |
| 計数データ | なし | なし | $\chi^2$検定 | 発生率の比較 |
|  | なし | あり | Poisson回帰 | 回帰モデルの一種 |
|  | あり | あり | 変量効果モデル、一般化推定方程式 | 回帰モデルの一種 |
| 生存時間データ | なし | なし | Kaplan-Meier曲線 | 生存曲線の記述 |
|  | なし | あり | Cox回帰 | 回帰モデルの一種 |
| 競合リスクデータ | なし | なし | Aalen-Johansen曲線 | 累積発生率曲線の記述 |
|  | なし | あり | Fine-Grayモデル | 回帰モデルの一種 |
:::

### 次のエピソードとRスクリプト

- [Understanding Collapsibility of Effect Measures: Marginal vs Stratified](logistic-regression-1.html)
- [From Risk to Logistic Regression](logistic-regression-2.html)
- [Logit: How a Transformation Shapes an Effect](logistic-regression-3.html)
- [Where My Logistic Regression Went Wrong](logistic-regression-4.html)
- [Why Logistic Regression Fails in Small Samples](logistic-regression-5.html)
- [Understanding Confounding in Effect Measures: Marginal vs Stratified](logistic-regression-6.html)
- [logistic-regression.R](../from-coffee-chat-to-r/logistic-regression.R)

### 過去のシリーズと用語集

- [A Story of Coffee Chat and Research Hypothesis](study-design-1.html)
- [Reading a Paper over a Cup of Coffee](frequentist-1.html)
- [Silent Confusions Hidden in Percentages](effects-1.html)
- [Statistical Terms in Plain Language](glossary.html)