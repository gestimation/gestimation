---
title: "Why Logistic Regression Fails in Small Samples"
format:
  html:
    toc: true
---

```{=html}
<style>
header#title-block-header.quarto-title-block {
  display: none !important;
}

body {
  margin: 0;
  background: #ffffff;
  color: #2b1f1a;
  font-family: "Inter", "Helvetica Neue", Arial, sans-serif;
  font-size: 18px;
  line-height: 1.55;
}

h1, h2, h3 {
  font-family: "Georgia", "Times New Roman", serif;
  color: #3a2a22;
  font-weight: 600;
}

h3 {
  margin-top: 2.4rem;
}

a {
  color: #8b4a24;
  text-decoration: none;
}
a:hover {
  color: #6f3718;
  text-decoration: underline;
}

.navbar {
  background: #ffffff !important;
  border-bottom: 1px solid #e6e0da;
  margin-bottom: 0 !important;
}
.navbar .navbar-brand,
.navbar .navbar-nav > li > a {
  color: #3a2a22 !important;
  border-bottom: 2px solid transparent;
  padding-bottom: 2px;
}
.navbar .navbar-nav > li > a:hover {
  border-bottom-color: #8b4a24;
}

.hero-text {
  padding: 1.5rem 0;
  color: #3a2a22;
}

.project-card {
  border: 1px solid #e6e0da;
  padding: 1.3rem 1.5rem;
  box-shadow: 0 1px 3px rgba(0,0,0,0.05);
  background: #ffffff;
  border-radius: 6px;
  margin-bottom: 1.5rem;
}

.about-section {
  border: 1px solid #e6e0da;
  padding: 1.5rem;
  border-radius: 0;
  background: #ffffff;
  margin-top: 2.5rem;
}

.about-section::after {
  content: "";
  display: block;
  clear: both;
}

.author-photo {
  float: left;
  margin-right: 1rem;
  border-radius: 50%;
}

.btn-link {
  display: inline-block;
  border: 1px solid #8b4a24;
  color: #8b4a24;
  padding: 0.4rem 0.9rem;
  font-size: 0.9rem;
  border-radius: 3px;
}
.btn-link:hover {
  background: #8b4a24;
  color: #ffffff;
}

.content-narrow {
  max-width: 1000px;
  margin: 0 auto;
  padding: 1.8rem 1.2rem 2.5rem;
}

.hero-header {
  margin-left: calc(50% - 50vw);
  margin-right: calc(50% - 50vw);
}

.hero-header-img {
  width: 100vw;
  height: min(400px, 45vh);
  object-fit: cover;
  object-position: 50% 40%;
  display: block;
}

.dialogue p {
  line-height: 1.7;
}
.dialogue .voice-father p {
  color: #7a3e1a !important;
}
.dialogue .voice-daughter p {
  color: #234f7c !important;
}
.dialogue .voice-daughter,
.dialogue .voice-father {
  margin-bottom: 0.4rem;
}

.text-right {
  text-align: right;
}
.serif-block p {
  font-family: "Georgia", "Times New Roman", serif;
  font-size: 0.9em;  line-height: 1.6;
}
</style>
```

::: hero-header
![](../image/father-daughter.png){.hero-header-img}
:::

# Adjusting for Bias IV − Why Logistic Regression Fails in Small Samples

::: {.serif-block .text-right}
Keywords: bias, effect measure, generalized linear model
:::

------------------------------------------------------------------------

### ロジスティック回帰の計算ができない理由

::::::::::::::::::::::::::::::::: dialogue
::: voice-daughter
私「ロジスティック回帰についてなんだけど。ロジット関数のグラフでイメージはわいたんだけどさ。がんサバイバー調査だと、ロジスティック回帰は2群を比較するとき使うんだよね」
::: 

::: voice-father
お父さん「そうだったね。じゃあ、続けてロジスティック回帰の下で、2人の対象者を比べるとどうなるかって話をするね」
::: 
:::::::::::::::::::::::::::::::::


::: {.callout-note appearance="simple" title="回帰係数とオッズ比の関係"}

共変量が1個のとき、ロジスティック回帰は

$\log(\frac{\pi_i}{1-\pi_i})=\beta_0+\beta_1X_{i}$

と表すことができます。そして、対象者$i=1$では$X_{1}=0$、対象者$i=2$では$X_{2}=1$という値をとる状況を考えてみましょう。それぞれの対象者の共変量の値を、上の式に代入すると、以下のようになります。

$\log(\frac{\pi_1}{1-\pi_1})=\beta_0$

$\log(\frac{\pi_2}{1-\pi_2})=\beta_0+\beta_1$

ここで、$\pi_1$は対象者1の確率パラメータ、$\pi_2$は対象者2の確率パラメータです。これらの確率パラメータから、オッズ比を求めるとどうなるでしょうか。それは、以下の式で計算されます。

$(\frac{\pi_2}{1-\pi_2})\div(\frac{\pi_1}{1-\pi_1})=\exp(\beta_0+\beta_1)\div \exp(\beta_0)=\exp(\beta_1)$

この結果は、回帰係数$\beta_1$の指数をとるとオッズ比が得られることを意味しています。

:::

::::::::::::::::::::::::::::::::: dialogue
::: voice-daughter
私「なるほど、回帰係数からオッズ比を計算できるってこういうことなのね」
::: 

::: voice-father
お父さん「このオッズ比の計算ができるのは、ロジット関数の特徴によるものでしょ。だから、ロジスティック回帰はオッズ比のモデルともいえるんだ」
::: 

::: voice-daughter
私「前の話だと、Cox回帰はハザード比のモデルなんだっけ？」
::: 

::: voice-father
お父さん「そういうこと。リスク比にはリスク比のモデルがあるし、リスク差にはリスク差のモデルがある。リンク関数はそれぞれ別の関数になる」
::: 

::: voice-daughter
私「じゃあ最後にさ、完全分離について説明してくれない？この前の解析で、なにが起きたのか知りたいんだよね」
::: 
::::::::::::::::::::::::::::::::: 

::: {.callout-note appearance="simple" title="ロジスティック回帰の推定"}

**推定に用いられる関数**

以前、ロジスティック回帰の$\pi_i$は2項分布の確率に対応しているといいました。2項分布の確率関数にはデータと$\pi_i$が含まれていますが、この式にデータを代入すると、$\pi_i$の式が得られます。さらに、$\pi_i$は$\beta_0,\beta_1,...,\beta_{p-1}$の関数です。したがって、確率関数の式は、$L(\beta_0,\beta_1,...,\beta_{p-1})$というように、$\beta_0,\beta_1,...,\beta_{p-1}$の関数とみなすことができます。この関数を尤度関数といいます。

ロジスティック回帰の回帰係数は、この尤度関数を最大化する値として計算されます。ただし、データから必ずしも解が求まるわけではありません。計算が収束しなかったり、不安定になったりする理由は、主に二通り考えられます。

**完全分離**

一つ目は、完全分離（complete separation）や擬似完全分離（quasi-complete separation）です。完全分離とは、

$\log(\frac{\hat\pi_i}{1-\hat\pi_i})=\hat\beta_0+\hat\beta_1X_{i,1}+...+\hat\beta_{p-1}X_{i,p-1}$

を計算したとき、その値によって、$Y_1,Y_2,...,Y_N$を100%の精度で0または1に判別できてしまう状況のことをいいます。擬似完全分離は、100%ではないものの、完全分離と同様に共変量からアウトカムが判別できてしまう状況です。別の言葉でいえば、共変量の情報に比べて、データに含まれる情報が不足していて、実質的にランダム性がないということを意味します。つまり、完全分離や擬似完全分離は、サンプルサイズに比べ、共変量の数が多すぎるときによく生じる問題です。

**多重共線性**

二つ目は、多重共線性（multicollinearity）です。これは、共変量同士の相関が強すぎることを意味します。仮に、年収が年齢に完全に比例する状況（つまり相関が1）を考えてみてください。このとき、ロジスティック回帰を用いて年収と年齢の影響を分離することはできません。これが多重共線性の一例です。言い換えると、相関の高い共変量は、推定が不安定になるため、同時にロジスティック回帰に含めない方がよい場合が多いです。
:::

::::::::::::::::::::::::::::::::: dialogue
::: voice-daughter
私「あれ？回帰分析って最小2乗法を使うんじゃないの？尤度関数？」
::: 

::: voice-father
お父さん「大学の教科書ではそう教わるけどね。最小2乗法を使うのは連続データのときだけかな。別の推定方法もいろいろある」
::: 

::: voice-daughter
私「ふーん。新しい方法がいろいろありそうってのは知ってるよ。AIとか機械学習とか」
::: 

::: voice-father
お父さん「まあね。はやりだよね」
::: 

::: voice-daughter
私「統計学と機械学習ってどう違うの？同じ？」
::: 

::: voice-father
お父さん「お隣さんって感じかな。データを解析するって意味では同じことをしているわけだし、機械学習の教科書でも、ロジスティック回帰とかロジスティック判別とかは扱うもの。統計学で習うROC曲線なんかは、機械学習の範疇な気がするし。重なる部分は多いよ。まあ、機械学習の方がたくさんのパラメータを推定する傾向にあるとはいえるかな。ロジスティック回帰よりニューラルネットワークの方が、モデルも複雑だし、パラメータの数も多い。さっき共変量の数を減らそうっていったでしょ。機械学習では、ああいうパラメータの数を減らすって発想はしない」
::: 

::: voice-daughter
私「パラメータの数が少ないのが統計学の特徴ってわけ？」
::: 

::: voice-father
お父さん「うーん、でもそれは程度問題かもしれない。一番の違いはなんだろうね。統計学は、統計的推測が基礎にあるってことかな。統計的推測っていうのは、標本から母集団を調べるっていう考え方のこと。手法でいうと、p値とか信頼区間とか、ああいうやつね。パラメータの数が多すぎると、これらの手法は性能が下がるんだ」
::: 

::: voice-daughter
私「機械学習に統計的推測って考え方はないの？」
::: 

::: voice-father
お父さん「機械学習やってる人も統計学は学んでいるから、ないとはいわないけど、あんまりこだわってないんじゃない？画像認識するとき、p値なんていらないでしょ」
::: 

::: voice-daughter
私「そういうもんかな。まあいいや、疲れてきたし。性別のオッズ比が無限大になった理由はわかったよ。尤度関数にデータを代入したら、回帰係数の解が変になるような式になったってわけね」
::: 

::: voice-father
お父さん「ん？思ったより芯を食った説明ができてるじゃない。さすがだね」
::: 

:::::::::::::::::::::::::::::::::


### 文献

- [田中司朗. 医学研究のための因果推論I. 一般化線型モデル. 東京: 朝倉書店; 2022](https://www.asakura.co.jp/detail.php?book_code=12270)

### エピソード、用語集、Rスクリプト

- [From Risk to Logistic Regression](logistic-regression-1.html)
- [Logit: How a Transformation Shapes an Effect](logistic-regression-2.html)
- [Where My Logistic Regression Went Wrong](logistic-regression-3.html)
- [Why Logistic Regression Fails in Small Samples](logistic-regression-4.html)
- [Simpson's paradox](logistic-regression-5.html)

- [Statistical Terms in Plain Language](glossary.html)

- [logistic-regression.R](../from-coffee-chat-to-r/logistic-regression.R)
