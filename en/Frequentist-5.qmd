# Story and Quiz ? Frequentist thinking V

Keywords: sample size calculation, clinical trial, observational study

---

### gSoc how many patients do I *actually* need?h

Dad: gHey, got a minute?h

Me: gWhat? You look unusually serious.h

Dad: gHave you calculated the **sample size** yet?h

Me: gSample sizec for what?h

Dad: gFor your **cancer survivor survey**.  
The number of patients youfre planning to include.h

Me: gAh, that. I was thinking of surveying **100 patients**.h

Dad: gLast time, we talked halfway through the topic.  
We covered **precision** and confidence intervals.  

Today I want to talk about:

> eHow many patients do we need  
> if we actually want to **detect a difference** between groups?f

In other words:  
**sample size for hypothesis testing**.h

Me: gOkay, continuation of the sample size story. Ifm listening.h

Dad: gPreviously we looked at sample size from the angle of  
**how wide a 95% confidence interval** you can tolerate.  

But if you want a study thatfs designed from the start to compare:

- **stoma vs no stoma**,  

and test a hypothesis about the **difference in return-to-work rate**,  
then you need to think in terms of:

- **p-values**,  
- ** error**, and  
- **ﾀ error**.h

Me: gHypothesis testingc so this is where p-values come back in?h

Dad: gExactly.  

If we go that route,  
we need to talk about how ** error, ﾀ error, and p-values** are connected.

Remember JCOG9502, the gastric cancer trial we talked about?  
Where did we leave off?h

Me: gWe talked about p-values and the ASA statement.  
But I donft remember hearing much about eerrorf other than random error.h

Dad: gRight. Then letfs start from there.  

Pass me that napkin and a pen, will you?h

Me: gThis one?h

Dad: gThanks.  
To explain  error and ﾀ error,  
I really need a little **2~2 table**.h

---

###  error and ﾀ error

Dad: gWefve been talking about JCOG9502 ?  
the trial comparing the **LTA** and **TH** surgical approaches.   

In that setting, we said there are two possible **truths**:

1. **LTA improves overall survival** compared with TH, or  
2. LTA has **no effect** on overall survival compared with TH.

On the other hand, the **result** of the trial (in a simplified view)  
can be either:

- **p <  (significant)**, or  
- **p ?  (not significant)**.

Combine these and you get a **2~2 table**:h

| Trial result                        | True: no effect (H?) | True: has effect (H?) |
|-------------------------------------|-----------------------|------------------------|
| Not significant (p ? )             | (correct)             | **ﾀ error**            |
| Significant (p < )                 | ** error**           | (power = 1 ? ﾀ)        |

Dad: gOut of these four cells,  
the ones we really worry about are the **two error cases**:

- There *is* a true effect, but the result is **not significant**  **ﾀ error**  
- There is **no true effect**, but the result comes out significant  ** error**

We call them:

- ** error (Type I error)**  
- **ﾀ error (Type II error)**  

and

> **Power** = 1 ? h

Me: gRight, intuitively that makes sense.

- It would be a **mistake** to say LTA is effective when it isnft.  
- It would also be a mistake to abandon a **truly useful** approach  
  just because it failed to reach significance.h

Dad: gExactly.  

But therefs an important point:

>  error and ﾀ error are **not** concepts that depend on p-values per se.  

You could decide eeffective / not effectivef  
based on something **other than** a p-value.

The key is this:

> When we use the rule ep < 0.05f,  
> we are choosing a rule that **controls  error** at 5%.  
> It says nothing about ﾀ error unless we also plan the sample size.h

---

###  error, ﾀ error, and p-values

Dad: gThink about the **probabilities** of  error and ﾀ error.

If we increase the **sample size**,  
random error gets smaller.  
Both  and ﾀ errors tend to get **smaller** as well ?  
itfs harder to be fooled.

But with **fixed sample size**,  
 and ﾀ have a **trade-off**:

- You canft make both very small at the same time  
  without increasing n.  
- If you insist on a **very small **,  
  ﾀ will typically get **bigger** unless you also increase n.

So in practice, we usually:

1. **Prioritize  error**,  
2. Choose a test procedure that guarantees  
    is not above some pre-specified level ?, and  
3. Call ? the **significance level**.

Using the rule ep < 0.05f  
is just shorthand for e = 0.05f.h

Me: gSo in JCOG9502,  
they didnft use 0.05, right?h

Dad: gRight.  
Because recruitment was difficult,  
they changed the plan so that:

- **one-sided  = 0.1**  
- **ﾀ = 0.2** (power = 0.8)

In other words,  
they allowed themselves to **compare the p-value to 0.1**.   

In that design:

- Even if LTA truly has **no survival benefit**,  
- About **10% of the time**  
  a trial like this would still declare eLTA is effectivef  
  just by chance.

So when you increase the **significance level** from 5% to 10%:

-  error **doubles**,  
- but the **required sample size** can be smaller.h

Me: gI get the table you drew on the napkin.  
But once you start talking about , ﾀ, and probabilities,  
I feel like Ifm losing the thread.h

Dad: gThen letfs translate them into **clinical consequences**.

In a trial like JCOG9502,  
where wefre checking if a **new treatment** is effective,  
you can think:

- ** error**  
   A treatment that is **no better than standard**  
    (or even worse) ends up being adopted.  
   Thatfs a **consumer risk**:  
    patients and society are hurt.

- **ﾀ error**  
   A treatment that **is actually effective**  
    is abandoned during development.  
   Thatfs a **producer risk**:  
    developers lose the chance to bring a good treatment to patients.h

Me: gFrom a doctorfs perspective,  
 error sounds more scary ?  
giving ineffective or harmful treatment to patients.h

Dad: gI feel the same.h

---

### Typical choices for  and ﾀ

Me: gI still donft have an intuitive feel for the numbers.  

Of course itfs better to have smaller error probabilities,  
but how small is esmall enoughf?h

Dad: gLook again at the JCOG9502 paper.  
What do they say about  and ﾀ?h

Me: gLet me seec  

> eThe amended sample size was 250,  
> with one-sided alpha error of 0.1 and beta error of 0.2.f

So:

- one-sided  = 0.1  
- ﾀ = 0.2 (power = 0.8)

And that led to a **total sample size of 250**.h

Dad: gExactly.

In many clinical trials,  
ﾀ is set to **0.1 or 0.2**,  
meaning **power = 0.9 or 0.8**.

The SWOG statisticians say in their oncology trials textbook:

> eBecause new treatments that truly work are rare,  
> we generally recommend a power of 90%.f

Which is eﾀ error = 0.1f.h   

Me: gThen for my study,  
canft I just say eLetfs also take 250 patientsf  
and call it a day?h

Dad: gThatfsc crude, but honestly not far off.  

Youfll see in a moment that  
your required sample size is in that ballpark.  

But instead of guessing,  
let me show you **actual sample size numbers**  
in a simple setting, and wefll decide more carefully.h

---

### Sample size tables for comparing two survival curves

Dad: gThere are many possible scenarios,  
but Ifll show you the most commonly used settings:

- **Two-sided  = 0.05**  
- **ﾀ = 0.2** (power = 0.8)  
- or **ﾀ = 0.1** (power = 0.9)

Wefll assume:

- two groups with equal size (allocation ratio 1:1),  
- and we compare their **survival probabilities** at a fixed time  
  using methods like the **log-rank test**.

The sample sizes Ifm about to show  
come from Machin et al.fs table for comparing two survival curves.   

We denote the survival probabilities in the two groups by:

- ﾎ? and ﾎ?.

In your research,  
these will correspond to **non-return-to-work probabilities**  
(i.e., 1 minus the return-to-work rate).h

---

#### Table 1. Sample sizes for comparing two survival curves (power 80%)

- Two-sided  = 0.05, power = 0.8 (ﾀ = 0.2)  
- Values are **total sample size** (sum of the two groups, 1:1 allocation)

| ﾎ? \ ﾎ? | 0.10 | 0.20 | 0.30 | 0.40 | 0.50 | 0.60 | 0.70 | 0.80 |
|--------:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|
| **0.15**|  964 |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.20**|  296 |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.25**|  156 | 1826 |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.30**|  102 |  506 |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.35**|   74 |  246 | 2486 |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.40**|   58 |  152 |  658 |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.45**|   48 |  104 |  308 | 2894 |   ?  |   ?  |   ?  |   ?  |
| **0.50**|   42 |   78 |  182 |  744 |   ?  |   ?  |   ?  |   ?  |
| **0.55**|   36 |   62 |  122 |  340 | 3034 |   ?  |   ?  |   ?  |
| **0.60**|   32 |   52 |   90 |  198 |  764 |   ?  |   ?  |   ?  |
| **0.65**|   28 |   42 |   70 |  130 |  342 | 2900 |   ?  |   ?  |
| **0.70**|   26 |   38 |   54 |   92 |  194 |  712 |   ?  |   ?  |
| **0.75**|   24 |   34 |   38 |   70 |  124 |  312 | 2492 |   ?  |
| **0.80**|   22 |   30 |   38 |   56 |   86 |  174 |  592 |   ?  |
| **0.85**|   22 |   26 |   34 |   46 |   66 |  110 |  254 | 1818 |
| **0.90**|   20 |   26 |   30 |   38 |   50 |  136 |  136 |  414 |

---

#### Table 2. Sample sizes for comparing two survival curves (power 90%)

- Two-sided  = 0.05, power = 0.9 (ﾀ = 0.1)  
- Values are **total sample size** (sum of the two groups, 1:1 allocation)

| ﾎ? \ ﾎ? | 0.10 | 0.20 | 0.30 | 0.40 | 0.50 | 0.60 | 0.70 | 0.80 |
|--------:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|
| **0.15**| 1290 |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.20**|  396 |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.25**|  208 | 2444 |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.30**|  136 |  676 |   ?  |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.35**|  100 |  330 | 3330 |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.40**|   78 |  202 |  880 |   ?  |   ?  |   ?  |   ?  |   ?  |
| **0.45**|   64 |  138 |  412 | 3876 |   ?  |   ?  |   ?  |   ?  |
| **0.50**|   54 |  104 |  242 |  996 |   ?  |   ?  |   ?  |   ?  |
| **0.55**|   46 |   82 |  162 |  454 | 4060 |   ?  |   ?  |   ?  |
| **0.60**|   42 |   68 |  120 |  264 | 1022 |   ?  |   ?  |   ?  |
| **0.65**|   38 |   56 |   90 |  172 |  456 | 3880 |   ?  |   ?  |
| **0.70**|   34 |   48 |   72 |  124 |  258 |  952 |   ?  |   ?  |
| **0.75**|   32 |   42 |   60 |   92 |  166 |  416 | 3336 |   ?  |
| **0.80**|   30 |   40 |   52 |   74 |  116 |  232 |  796 |   ?  |
| **0.85**|   28 |   34 |   46 |   60 |   88 |  146 |  338 | 2436 |
| **0.90**|   26 |   32 |   38 |   50 |   68 |  100 |  180 |  548 |

---

### Mapping this to the return-to-work study

Dad: gNow, back to your project.

For the cancer survivor study,  
we have two groups:

- **WITH STOMA**  
- **WITHOUT STOMA**

Ifm going to treat ereturned to workf as the **event**,  
and analyze **time to return-to-work** as a **survival outcome**.

Letfs think in terms of **1-year follow-up**.

In the tables:

- ﾎ? and ﾎ? are **survival probabilities** at 1 year.  

For your study,  
that corresponds to the **probability of *not* returning to work**  
within 1 year in each group.h

Me: gSo if the 1-year return-to-work rates are:

- 80% in the non-stoma group,  
- 60% in the stoma group,

then:

- non-return probabilities are 0.2 and 0.4,

so we plug in **ﾎ? = 0.2, ﾎ? = 0.4**.h

Dad: gExactly.  
Now look at Table 2 (power 90%), at ﾎ? = 0.2, ﾎ? = 0.4.h

Me: gI see **202** there.  

So we need **202 patients total**?h

Dad: gYes ? in this simplified setting.h

Me: gAnd if we can tolerate power 80%?h

Dad: gThen in Table 1,  
for ﾎ? = 0.2, ﾎ? = 0.4,  
youfll find **152**.

So:

- power 90%  about **202 patients**  
- power 80%  about **152 patients**h

Me: gHmmc  
Still feels big. I was thinking e100fch

---

### Survival curves, log-rank test, and Cox regression

Me: gIfm still a bit confused.

In JCOG9502, they compared **two survival curves**, right?h

Dad: gRight.h

Me: gIn my study, wefre interested in the relationship between:

- stoma (yes/no) and  
- return-to-work status.

Earlier, you told me:

- for **binary data**, we often use **logistic regression**  
- for **survival data**, we use **Cox regression**

But these tables are about comparing **survival curves**.  
Is that the same as running a **Cox regression**?h

Dad: gGood question.

The p-value for comparing two survival curves  
can be obtained in several ways:

- **Log-rank test**  
- **Cox proportional hazards model**, and others.

The tables wefre using here  
were originally made for the **log-rank test**.

However, if the only covariate is **group (stoma vs no stoma)**,  
then:

- the log-rank test and  
- the Cox model

give almost the **same result**,  
especially with a **moderate or large sample size**.

So itfs fine to think of these tables  
as roughly applicable to the Cox model too.h

Me: gOkay, so in the simplest two-group comparison,  
log-rank and Cox are basically aligned.h

---

### What if group sizes are not 1:1?

Me: gTherefs something else.

The tables say eallocation ratio 1:1f.  
That means equal numbers in the two groups, right?h

Dad: gRight.  
Most randomized trials aim for 1:1 allocation.h

Me: gBut in an observational study of cancer survivors,  
Ifm guessing:

- patients **with stoma** will be fewer than  
- patients **without stoma**.

Maybe stoma is about **half** as common?h

Dad: gExactly.  
In **surveys and observational studies**,  
group sizes are usually **not** equal.

The tables I showed are based on 1:1 allocation,  
but when the settings get more complex,  
you normally use:

- the **software** supplied with the book, or  
- an R package.

Here we can use the R package **powerSurvEpi**.   

Letfs assume:

- Return-to-work rate in stoma group = **60%**  
- Return-to-work rate in non-stoma group = **80%**

Then the 1-year non-return probabilities are:

- pE = 0.4 (event = not returning; here wefre parameterizing via return)  
- pC = 0.2

To go from these probabilities to a hazard ratio,  
we can use the relationship between survival probability and hazard  
from the exponential model:

> survival = (1 ? p) = exp(?ﾉt)

and rearrange to get a rough hazard ratio from the two probabilities.h

---

### Sample size design using the `powerSurvEpi` package

```r
# install.packages("powerSurvEpi") # if needed
library(powerSurvEpi)

ssizeCT.default(
  power = 0.9,
  k     = 1,                      # allocation ratio (nE : nC)
  pE    = 0.6,                    # return-to-work rate in stoma group
  pC    = 0.8,                    # return-to-work rate in non-stoma group
  RR    = log(1 - 0.8) / log(1 - 0.6),  # approximate HR from return rates
  alpha = 0.05
)



# Example output:
#   nE  nC
#  100 100
Me: gSo with power = 0.9 and equal group sizes (k = 1),
we need 100 stoma patients and 100 non-stoma patients,
total 200.h

Dad: gRight.

Now letfs change the allocation ratio:

k = 0.5  2:1 (non-stoma : stoma)

k = 0.25  4:1h

r
R[hRs[・
ssizeCT.default(
  power = 0.9,
  k     = 0.5,                  # allocation ratio 1:2
  pE    = 0.6,
  pC    = 0.8,
  RR    = log(1 - 0.8) / log(1 - 0.6),
  alpha = 0.05
)
r
R[hRs[・
#   nE  nC
#    59 118
r
R[hRs[・
ssizeCT.default(
  power = 0.9,
  k     = 0.25,                 # allocation ratio 1:4
  pE    = 0.6,
  pC    = 0.8,
  RR    = log(1 - 0.8) / log(1 - 0.6),
  alpha = 0.05
)
r
R[hRs[・
#   nE  nC
#    41 161
Dad: gSo when:

k = 1 (1:1): nE + nC = 100 + 100 = 200

k = 0.5 (1:2): nE + nC = 59 + 118 = 177

k = 0.25 (1:4): nE + nC = 41 + 161 = 202

The total sample size shifts a bit,
but you still need plenty of patients.h

Me: gWowc
The sample size is larger than I expected.
Ifll have to think about adding more participating hospitals.h

Dad: gI know the feeling.

When you design a study strictly from a frequentist perspective
? controlling  and ﾀ and using proper sample size calculations ?
people almost always say:

eTherefs no way we can recruit that manycfh

Me: gcThatfs exactly my reaction.h

Dad: gBut now that you understand what , ﾀ, power,
and sample size mean,

youfre starting to say:

eItfs tough, but if I want a study
to have real statistical value,
then I have to aim for these numbers.f

Thatfs a key mindset shift.h

Me: gTrue.
If you imagine your study being repeated 1000 times,
and in 200 of those the result fails to detect a true effect
just because it was underpoweredc

Ifd hate to be that one failed realization.h

Dad: gExactly.
Frequentist thinking is about imagining
those repeated experiments in your head.

Once you internalize that,
you start to see what it means
for a study to be statistically trustworthy.h

References
Machin D, Campbell MJ, Tan SB, Tan SH.
Sample Size Tables for Clinical Studies, 4th ed.
(Japanese edition: 繩ŵ߂̃TvTCY݌v). Kyoto University Press; 2022.

Sasako M, Sano T, Yamamoto S, Sairenji M, Arai K, Kinoshita T, Nashimoto A, Hiratsuka M; Japan Clinical Oncology Group (JCOG9502).
Left thoracoabdominal approach versus abdominal-transhiatal approach for gastric cancer of the cardia or subcardia:
a randomised controlled trial. Lancet Oncol. 2006;7(8):644?651.

End of the episode on Frequentist thinking
This is the last episode of the Frequentist thinking mini-series.

From here, you now have:

an intuition for p-values and what they are not,

a feel for 95% CIs as long-run properties of methods,

and a practical sense of , ﾀ, power, and sample size.

In the next series,
the father and daughter will move on to:

Effects and time ?
where we think more deeply about
what kind of effect we are estimating
and at which time points it matters.